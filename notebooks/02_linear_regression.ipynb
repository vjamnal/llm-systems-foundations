{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 \u00b7 Linear Regression with PyTorch\n",
    "\n",
    "In this notebook we train a linear regression model from scratch using the `lsf` library.\n",
    "\n",
    "Topics covered:\n",
    "1. Generating synthetic data\n",
    "2. Defining a `nn.Module` model\n",
    "3. Training loop (loss \u2192 backward \u2192 step)\n",
    "4. Evaluating with MSE, MAE and R\u00b2\n",
    "5. Visualising predictions vs. ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')   # allow importing lsf when running from notebooks/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lsf import (\n",
    "    Config,\n",
    "    LinearRegression,\n",
    "    make_regression_data,\n",
    "    mae,\n",
    "    mse,\n",
    "    r2_score,\n",
    "    set_seed,\n",
    "    train_one_epoch,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "print(f'PyTorch {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_samples=500,\n",
    "    n_features=1,\n",
    "    noise=0.3,\n",
    "    lr=0.05,\n",
    "    epochs=80,\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    ")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seed & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, true_w, true_b = make_regression_data(\n",
    "    n_samples=cfg.n_samples,\n",
    "    n_features=cfg.n_features,\n",
    "    noise=cfg.noise,\n",
    "    test_size=cfg.test_size,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(f'true weight: {true_w.squeeze().item():.4f}')\n",
    "print(f'true bias  : {true_b.item():.4f}')\n",
    "print(f'train batches: {len(train_loader)}  |  test samples: {cfg.n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise training data (works for n_features=1)\n",
    "X_all, y_all = [], []\n",
    "for xb, yb in train_loader:\n",
    "    X_all.append(xb)\n",
    "    y_all.append(yb)\n",
    "X_all = torch.cat(X_all).squeeze().numpy()\n",
    "y_all = torch.cat(y_all).squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_all, y_all, s=10, alpha=0.5, label='train')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Training data')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model, Loss & Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = LinearRegression(in_features=cfg.n_features).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:3d}/{cfg.epochs}  loss={loss:.6f}')\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.title('Training loss curve')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds, targets = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Test MSE : {mse(preds, targets):.6f}')\n",
    "print(f'Test MAE : {mae(preds, targets):.6f}')\n",
    "print(f'Test R\u00b2  : {r2_score(preds, targets):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity plot: predictions vs. ground truth\n",
    "p = preds.squeeze().numpy()\n",
    "t = targets.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(t, p, s=15, alpha=0.6)\n",
    "lim = [min(t.min(), p.min()) - 0.2, max(t.max(), p.max()) + 0.2]\n",
    "plt.plot(lim, lim, 'r--', linewidth=1)\n",
    "plt.xlabel('True'); plt.ylabel('Predicted')\n",
    "plt.title('Parity plot (test set)')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learned Parameters vs. True Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_w = model.linear.weight.detach().cpu().item()\n",
    "learned_b = model.linear.bias.detach().cpu().item()\n",
    "\n",
    "print(f'           weight        bias')\n",
    "print(f'True    :  {true_w.item():.4f}        {true_b.item():.4f}')\n",
    "print(f'Learned :  {learned_w:.4f}        {learned_b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We trained a linear regression model using gradient descent and verified that the learned\n",
    "parameters converge close to the true data-generating parameters.\n",
    "\n",
    "Key takeaways:\n",
    "- `nn.Module` + `nn.Linear` encapsulate model parameters.\n",
    "- `loss.backward()` computes gradients via autograd.\n",
    "- `optimizer.step()` updates parameters using those gradients.\n",
    "- R\u00b2 close to 1 means the model explains most of the variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}